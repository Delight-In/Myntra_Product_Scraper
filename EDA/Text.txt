Performing **Exploratory Data Analysis (EDA)** on text data requires different techniques compared to numerical or categorical data. Here's a comprehensive list of 100 EDA techniques to explore text data, divided into **general overview**, **text statistics**, **visualization**, **text preprocessing**, **sentiment analysis**, **topic modeling**, **keyword extraction**, and **modeling**:

---

### **1. General Overview of Text Data:**

1. **Check the data shape**
   ```python
   df.shape
   ```

2. **Display a few records**
   ```python
   df.head()
   ```

3. **Check for null values**
   ```python
   df.isnull().sum()
   ```

4. **Check for duplicate rows**
   ```python
   df.duplicated().sum()
   ```

5. **Check text length (number of characters)**
   ```python
   df['text_length'] = df['text'].apply(len)
   ```

6. **Check for empty strings**
   ```python
   df[df['text'].str.strip() == '']
   ```

7. **Inspect data types**
   ```python
   df.dtypes
   ```

8. **Check the number of unique text entries**
   ```python
   df['text'].nunique()
   ```

9. **Check the number of words in each document**
   ```python
   df['word_count'] = df['text'].apply(lambda x: len(x.split()))
   ```

10. **Check the number of characters in each document**
    ```python
    df['char_count'] = df['text'].apply(len)
    ```

---

### **2. Text Statistics:**

11. **Calculate the average text length**
    ```python
    df['text_length'].mean()
    ```

12. **Calculate the average word count**
    ```python
    df['word_count'].mean()
    ```

13. **Find the maximum word count in a document**
    ```python
    df['word_count'].max()
    ```

14. **Find the minimum word count in a document**
    ```python
    df['word_count'].min()
    ```

15. **Distribution of text lengths**
    ```python
    df['text_length'].hist()
    ```

16. **Distribution of word counts**
    ```python
    df['word_count'].hist()
    ```

17. **Text length vs. word count correlation**
    ```python
    df[['text_length', 'word_count']].corr()
    ```

18. **Top 10 most frequent words**
    ```python
    from collections import Counter
    word_counts = Counter(" ".join(df['text']).split())
    word_counts.most_common(10)
    ```

19. **Average word length**
    ```python
    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))
    ```

20. **Unique words count**
    ```python
    df['unique_word_count'] = df['text'].apply(lambda x: len(set(x.split())))
    ```

---

### **3. Text Visualization:**

21. **Word Cloud of the most frequent words**
    ```python
    from wordcloud import WordCloud
    wordcloud = WordCloud().generate(" ".join(df['text']))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()
    ```

22. **Bar Plot of Top N Words**
    ```python
    import matplotlib.pyplot as plt
    top_words = word_counts.most_common(20)
    words, counts = zip(*top_words)
    plt.bar(words, counts)
    plt.xticks(rotation=90)
    plt.show()
    ```

23. **Text Length Distribution (Histogram)**
    ```python
    df['text_length'].plot(kind='hist', bins=50)
    ```

24. **Word Frequency Distribution**
    ```python
    sns.histplot(list(word_counts.values()), bins=50)
    ```

25. **Word Cloud by Category (if available)**
    ```python
    for category in df['category'].unique():
        text = " ".join(df[df['category'] == category]['text'])
        wordcloud = WordCloud().generate(text)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(category)
        plt.axis('off')
        plt.show()
    ```

26. **Sentence Length Distribution**
    ```python
    df['sentence_count'] = df['text'].apply(lambda x: len(x.split('.')))
    df['sentence_count'].plot(kind='hist', bins=50)
    ```

27. **Top N Bigrams/Trigrams**
    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english')
    X = vectorizer.fit_transform(df['text'])
    bigrams = vectorizer.get_feature_names_out()
    ```

28. **Bigram Cloud**
    ```python
    bigram_words = " ".join([" ".join(bigram) for bigram in bigrams])
    wordcloud = WordCloud().generate(bigram_words)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()
    ```

29. **Term Frequency-Inverse Document Frequency (TF-IDF) Visualization**
    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(stop_words='english')
    X = tfidf.fit_transform(df['text'])
    df_tfidf = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names_out())
    sns.heatmap(df_tfidf.corr())
    ```

30. **Cluster Words using PCA or t-SNE**
    ```python
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(X.toarray())
    plt.scatter(reduced[:, 0], reduced[:, 1], c=df['category'].apply(lambda x: hash(x)%10))
    ```

---

### **4. Text Preprocessing:**

31. **Remove Stop Words**
    ```python
    from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
    df['text_cleaned'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in ENGLISH_STOP_WORDS]))
    ```

32. **Lowercase Conversion**
    ```python
    df['text_cleaned'] = df['text'].str.lower()
    ```

33. **Remove Punctuation**
    ```python
    df['text_cleaned'] = df['text'].str.replace('[^\w\s]', '')
    ```

34. **Remove Numbers**
    ```python
    df['text_cleaned'] = df['text'].str.replace(r'\d+', '')
    ```

35. **Tokenization**
    ```python
    from nltk.tokenize import word_tokenize
    df['tokens'] = df['text'].apply(lambda x: word_tokenize(x))
    ```

36. **Stemming**
    ```python
    from nltk.stem import PorterStemmer
    stemmer = PorterStemmer()
    df['stemmed_text'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])
    ```

37. **Lemmatization**
    ```python
    from nltk.stem import WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()
    df['lemmatized_text'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
    ```

38. **Remove Special Characters**
    ```python
    df['text_cleaned'] = df['text'].str.replace(r'\W', ' ')
    ```

39. **Remove Extra Spaces**
    ```python
    df['text_cleaned'] = df['text_cleaned'].str.replace(r'\s+', ' ')
    ```

40. **Spelling Correction**
    ```python
    from textblob import TextBlob
    df['corrected_text'] = df['text'].apply(lambda x: str(TextBlob(x).correct()))
    ```

---

### **5. Sentiment Analysis:**

41. **Sentiment Polarity (Positive/Negative/Neutral)**
    ```python
    from textblob import TextBlob
    df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)
    ```

42. **Categorize Sentiment (Positive, Negative, Neutral)**
    ```python
    df['sentiment_label'] = df['sentiment'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))
    ```

43. **Sentiment Distribution Plot**
    ```python
    sns.countplot(df['sentiment_label'])
    ```

44. **VADER Sentiment Analysis**
    ```python
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
    sid = SentimentIntensityAnalyzer()
    df['vader_sentiment'] = df['text'].apply(lambda x: sid.polarity_scores(x)['compound'])
    ```

45. **VADER Sentiment Labeling**
    ```python
    df['vader_label'] = df['vader_sentiment'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))
    ```

46. **Comparison of Sentiment Scores (TextBlob vs. VADER)**
    ```python
    df[['sentiment', 'vader_sentiment']].plot(kind='scatter', x='sentiment', y='vader_sentiment')
    ```

47. **Text Sentiment by Category**
    ```python
    sns.boxplot(x='category', y='sentiment', data=df)
    ```

48. **Sentiment Over Time (if applicable)**
    ```python
    df['date'] = pd.to_datetime(df['date'])
    df.groupby(df['date'].dt.month)['sentiment'].mean().plot()


    ```

49. **Wordcloud by Sentiment**
    ```python
    positive_text = " ".join(df[df['sentiment_label'] == 'positive']['text'])
    wordcloud = WordCloud().generate(positive_text)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()
    ```

50. **Find Most Positive and Negative Reviews (Based on Sentiment)**
    ```python
    most_positive = df.loc[df['sentiment'].idxmax()]
    most_negative = df.loc[df['sentiment'].idxmin()]
    ```

---

### **6. Topic Modeling:**

51. **Topic Modeling with Latent Dirichlet Allocation (LDA)**
    ```python
    from sklearn.decomposition import LatentDirichletAllocation
    vectorizer = CountVectorizer(stop_words='english')
    X = vectorizer.fit_transform(df['text'])
    lda = LatentDirichletAllocation(n_components=3)
    lda.fit(X)
    ```

52. **Display Top Words in Each Topic**
    ```python
    for topic_idx, topic in enumerate(lda.components_):
        print(f"Topic {topic_idx}:")
        print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]])
    ```

53. **Topic Visualization with pyLDAvis**
    ```python
    import pyLDAvis.sklearn
    pyLDAvis.sklearn.prepare(lda, X, vectorizer)
    ```

54. **Determine Optimal Number of Topics (using Perplexity)**
    ```python
    lda_model = LatentDirichletAllocation(n_components=10)
    lda_model.fit(X)
    print(f"Perplexity: {lda_model.perplexity(X)}")
    ```

55. **KMeans Clustering for Topic Detection**
    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=3)
    kmeans.fit(X)
    df['topic'] = kmeans.labels_
    ```

56. **Word Cloud for Each Topic**
    ```python
    for topic_idx, topic in enumerate(lda.components_):
        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]
        topic_words = " ".join(topic_words)
        wordcloud = WordCloud().generate(topic_words)
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f"Topic {topic_idx}")
        plt.show()
    ```

57. **TruncatedSVD (for dimensionality reduction in LDA topics)**
    ```python
    from sklearn.decomposition import TruncatedSVD
    svd = TruncatedSVD(n_components=2)
    svd_result = svd.fit_transform(X)
    ```

58. **Top N Keywords per Topic**
    ```python
    df.groupby('topic')['text'].apply(lambda x: ' '.join(x)).head()
    ```

---

### **7. Keyword Extraction:**

59. **TF-IDF Keyword Extraction**
    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer
    tfidf = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf.fit_transform(df['text'])
    ```

60. **Extract Keywords with TF-IDF**
    ```python
    feature_names = tfidf.get_feature_names_out()
    dense = tfidf_matrix.todense()
    keywords = pd.DataFrame(dense, columns=feature_names)
    ```

61. **Keyword Frequency Distribution**
    ```python
    keyword_counts = keywords.sum(axis=0).sort_values(ascending=False).head(10)
    ```

62. **RAKE (Rapid Automatic Keyword Extraction)**
    ```python
    from rake_nltk import Rake
    rake = Rake()
    df['keywords'] = df['text'].apply(lambda x: rake.extract_keywords_from_text(x) or [])
    ```

63. **Top N Keywords using RAKE**
    ```python
    df['keywords'].apply(lambda x: x[:5]).head()
    ```

64. **Topic Wordcloud**
    ```python
    topic_keywords = " ".join(keywords.head(20).index)
    wordcloud = WordCloud().generate(topic_keywords)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()
    ```

---

### **8. Model Building:**

65. **Train-Test Split**
    ```python
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2)
    ```

66. **Text Vectorization (CountVectorizer)**
    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    vectorizer = CountVectorizer(stop_words='english')
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    ```

67. **Text Vectorization (TF-IDF)**
    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer
    vectorizer = TfidfVectorizer(stop_words='english')
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    ```

68. **Logistic Regression Model**
    ```python
    from sklearn.linear_model import LogisticRegression
    clf = LogisticRegression()
    clf.fit(X_train_vec, y_train)
    ```

69. **Random Forest for Text Classification**
    ```python
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier()
    rf.fit(X_train_vec, y_train)
    ```

70. **Support Vector Machine for Text Classification**
    ```python
    from sklearn.svm import SVC
    svm = SVC(kernel='linear')
    svm.fit(X_train_vec, y_train)
    ```

71. **Naive Bayes for Text Classification**
    ```python
    from sklearn.naive_bayes import MultinomialNB
    nb = MultinomialNB()
    nb.fit(X_train_vec, y_train)
    ```

72. **K-Nearest Neighbors (KNN) for Text Classification**
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier()
    knn.fit(X_train_vec, y_train)
    ```

73. **Model Evaluation with Accuracy**
    ```python
    y_pred = clf.predict(X_test_vec)
    accuracy = accuracy_score(y_test, y_pred)
    ```

74. **Model Evaluation with Confusion Matrix**
    ```python
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.show()
    ```

75. **Model Evaluation with Precision, Recall, and F1-Score**
    ```python
    from sklearn.metrics import precision_score, recall_score, f1_score
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')
    ```

76. **Cross-validation**
    ```python
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(clf, X_train_vec, y_train, cv=5)
    ```

77. **Hyperparameter Tuning (GridSearchCV)**
    ```python
    from sklearn.model_selection import GridSearchCV
    param_grid = {'C': [0.1, 1, 10]}
    grid = GridSearchCV(SVC(), param_grid)
    grid.fit(X_train_vec, y_train)
    ```

78. **Model Performance with ROC Curve**
    ```python
    from sklearn.metrics import roc_curve
    fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(X_test_vec)[:, 1])
    plt.plot(fpr, tpr)
    plt.show()
    ```

---

### **9. Final Insights:**

79. **Compare Model Performance**
    ```python
    models = {'Logistic Regression': clf, 'Random Forest': rf, 'SVM': svm}
    for name, model in models.items():
        print(f"{name}: {model.score(X_test_vec, y_test)}")
    ```

80. **Feature Importance Visualization (Random Forest)**
    ```python
    importances = rf.feature_importances_
    feature_names = vectorizer.get_feature_names_out()
    sorted_idx = np.argsort(importances)[::-1]
    plt.bar(range(len(sorted_idx)), importances[sorted_idx], tick_label=[feature_names[i] for i in sorted_idx])
    plt.show()
    ```

81. **Use Pretrained Embeddings (Word2Vec, GloVe)**
    ```python
    from gensim.models import Word2Vec
    model = Word2Vec(df['text'].apply(lambda x: x.split()))
    embeddings = model.wv
    ```

82. **Text Classification with Deep Learning (RNN/LSTM)**
    ```python
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, LSTM, Dense
    model = Sequential()
    model.add(Embedding(input_dim=vocab_size, output_dim=128))
    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    ```

83. **Text Generation with RNN**
    ```python
    # Prepare sequence data, tokenization
    # Define an RNN model to generate text
    ```

84. **Use BERT for Text Classification**
    ```python
    from transformers import BertTokenizer, BertForSequenceClassification
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
    ```

85. **Use Pretrained BERT for Sentiment Analysis**
    ```python
    from transformers import pipeline
    sentiment_analyzer = pipeline('sentiment-analysis')
    sentiment = sentiment_analyzer("This is a great day!")
    ```

---

### **10. Miscellaneous Techniques:**

86. **Entity Recognition (NER)**
    ```python
    import spacy
    nlp = spacy.load('en_core_web_sm')
    doc = nlp("Barack Obama was born in Hawaii.")
    for ent in doc.ents:
        print(ent.text, ent.label_)
    ```

87. **Topic Distribution Over Documents**
    ```python
    import pandas as pd
    topics = lda.transform(X)
    topic_df = pd.DataFrame(topics, columns=[f"Topic {i}" for i in range(lda.n_components)])
    ```

88. **Textual Entropy Calculation**
    ```python
    import numpy as np
    from collections import Counter
    text = " ".join(df['text'])
    freq = Counter(text.split())
    entropy = -sum([p * np.log2(p) for p in (np.array(list(freq.values())) / sum(freq.values()))])
    ```

89. **Emotion Classification (using NRC Emotion Lexicon)**
    ```python
    from nrclex import NRCLex
    df['emotion'] = df['text'].apply(lambda x: NRCLex(x).top_emotions())
    ```

90. **Extract Named Entities and Visualize**
    ```python
    df['named_entities'] = df['text'].apply(lambda x: [ent.text for ent in nlp(x).ents])
    ```

91. **Text Preprocessing Pipeline**
    ```python
    def preprocess(text):
        text = text.lower()
        text = re.sub(r'\d+', '', text)
        text = re.sub(r'[^\w\s]', '', text)
        return text
    df['cleaned_text'] = df['text'].apply(preprocess)
    ```

92. **Custom Tokenization with Regex**
    ```python
    df['tokens'] = df['text'].apply(lambda x: re.findall(r'\b\w+\b', x.lower()))
    ```

93. **Keyword Extraction with YAKE**
    ```python
    from yake import KeywordExtractor
    extractor = KeywordExtractor()
    keywords = extractor.extract_keywords(df['text'].iloc[0])
    ```

94. **Text Length Analysis by Category**
    ```python
    df.groupby('category')['text_length'].describe()
    ```

95. **Text Mining from PDFs or Docs**
    ```python
    from PyPDF2 import PdfReader
    reader = PdfReader("document.pdf")
    text = ''.join([page.extract_text() for page in reader.pages])
    ```

96. **Exploration of Part-of-Speech Tags**
    ```python
    from nltk import pos_tag
    df['pos_tags'] = df['tokens'].apply(lambda x: pos_tag(x))
    ```

97. **Document Similarity Calculation (Cosine Similarity)**
    ```python
    from sklearn.metrics.pairwise import cosine_similarity
    cosine_similarity(X[0], X[1])
    ```

98. **N-grams Creation**
    ```python
    from sklearn.feature_extraction.text import CountVectorizer
    ngrams = CountVectorizer(ngram_range=(2, 2)).fit_transform(df['text'])
    ```

99. **Summarization of Text with Gensim**
    ```python
    from gensim.summarization import summarize
    summarize(df['text'].iloc[0])
    ```

100. **Detect Language Using Langdetect**
    ```python
    from langdetect import detect
    df['language'] = df['text'].apply(lambda x: detect(x))
    ```

---

This list should help you perform comprehensive EDA on **text data**, focusing on various techniques that span from basic text processing, sentiment analysis, and topic modeling to more advanced techniques like deep learning and keyword extraction.


