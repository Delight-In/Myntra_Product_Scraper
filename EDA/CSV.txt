Here is the Python code for performing each of the EDA tasks mentioned earlier. This code uses common libraries such as `pandas`, `matplotlib`, `seaborn`, and `scipy` to perform the EDA tasks.

### **EDA on Numerical Data**

#### **For Continuous Features:**

1. **Summary Statistics**
   ```python
   df.describe()
   ```

2. **Range**
   ```python
   df['feature'].max() - df['feature'].min()
   ```

3. **Distribution Shape (Histogram)**
   ```python
   import matplotlib.pyplot as plt
   df['feature'].hist()
   plt.show()
   ```

4. **Density Plot**
   ```python
   import seaborn as sns
   sns.kdeplot(df['feature'])
   plt.show()
   ```

5. **Boxplot**
   ```python
   sns.boxplot(x=df['feature'])
   plt.show()
   ```

6. **Violin Plot**
   ```python
   sns.violinplot(x=df['feature'])
   plt.show()
   ```

7. **Skewness**
   ```python
   df['feature'].skew()
   ```

8. **Kurtosis**
   ```python
   df['feature'].kurt()
   ```

9. **Correlation Matrix**
   ```python
   df.corr()
   sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
   plt.show()
   ```

10. **Pair Plot**
    ```python
    sns.pairplot(df)
    plt.show()
    ```

11. **Scatter Plot**
    ```python
    sns.scatterplot(x=df['feature1'], y=df['feature2'])
    plt.show()
    ```

12. **Heatmap (Correlation)**
    ```python
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
    plt.show()
    ```

13. **Normality Test (Shapiro-Wilk)**
    ```python
    from scipy.stats import shapiro
    stat, p = shapiro(df['feature'])
    print(f'Statistic={stat}, p-value={p}')
    ```

14. **Outlier Detection (IQR)**
    ```python
    Q1 = df['feature'].quantile(0.25)
    Q3 = df['feature'].quantile(0.75)
    IQR = Q3 - Q1
    outliers = df[(df['feature'] < (Q1 - 1.5 * IQR)) | (df['feature'] > (Q3 + 1.5 * IQR))]
    ```

15. **Outlier Detection (Z-Score)**
    ```python
    from scipy.stats import zscore
    df['z_score'] = zscore(df['feature'])
    outliers = df[df['z_score'].abs() > 3]
    ```

16. **Transformation (Log)**
    ```python
    df['log_feature'] = df['feature'].apply(lambda x: np.log(x + 1))
    ```

17. **Rescaling**
    ```python
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    df['scaled_feature'] = scaler.fit_transform(df[['feature']])
    ```

18. **Rolling Mean**
    ```python
    df['rolling_mean'] = df['feature'].rolling(window=10).mean()
    ```

19. **Autocorrelation**
    ```python
    from pandas.plotting import autocorrelation_plot
    autocorrelation_plot(df['feature'])
    plt.show()
    ```

20. **Binned Histograms**
    ```python
    df['binned_feature'] = pd.cut(df['feature'], bins=10)
    df['binned_feature'].value_counts().plot(kind='bar')
    plt.show()
    ```

21. **Time Series Decomposition**
    ```python
    from statsmodels.tsa.seasonal import seasonal_decompose
    decomposition = seasonal_decompose(df['feature'], model='additive', period=12)
    decomposition.plot()
    plt.show()
    ```

22. **Scatter Matrix**
    ```python
    pd.plotting.scatter_matrix(df, figsize=(10, 10))
    plt.show()
    ```

23. **Cumulative Distribution Function (CDF)**
    ```python
    sns.ecdfplot(df['feature'])
    plt.show()
    ```

24. **Outlier Detection (Modified Z-Score)**
    ```python
    from statsmodels import robust
    df['mad'] = robust.mad(df['feature'])
    df['modified_z_score'] = 0.6745 * (df['feature'] - df['feature'].median()) / df['mad']
    outliers = df[df['modified_z_score'].abs() > 3.5]
    ```

25. **Trend Lines**
    ```python
    sns.regplot(x=df['feature1'], y=df['feature2'])
    plt.show()
    ```

26. **Cross-correlation**
    ```python
    import numpy as np
    np.corrcoef(df['feature1'], df['feature2'])
    ```

27. **Logistic Transformation**
    ```python
    df['logistic_transformed'] = 1 / (1 + np.exp(-df['feature']))
    ```

28. **PCA (Principal Component Analysis)**
    ```python
    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    pca_result = pca.fit_transform(df[['feature1', 'feature2']])
    ```

29. **Manhattan Distance**
    ```python
    from scipy.spatial.distance import cityblock
    cityblock(df['feature1'], df['feature2'])
    ```

30. **Silhouette Score**
    ```python
    from sklearn.metrics import silhouette_score
    silhouette_score(df[['feature1', 'feature2']], labels)
    ```

31. **Fourier Transform**
    ```python
    from numpy.fft import fft
    fft_result = fft(df['feature'])
    ```

32. **Variance Inflation Factor (VIF)**
    ```python
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    vif_data = pd.DataFrame()
    vif_data['feature'] = df.columns
    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    ```

33. **Cumulative Sum (CUSUM)**
    ```python
    df['cusum'] = df['feature'].cumsum()
    ```

34. **Linear Regression**
    ```python
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(df[['feature1']], df['feature2'])
    ```

35. **Quartile Analysis**
    ```python
    Q1 = df['feature'].quantile(0.25)
    Q2 = df['feature'].quantile(0.50)
    Q3 = df['feature'].quantile(0.75)
    ```

36. **Heatmap of Rolling Statistics**
    ```python
    rolling_stats = df['feature'].rolling(window=10).mean()
    sns.heatmap(rolling_stats, cmap='coolwarm')
    plt.show()
    ```

37. **Trend Analysis**
    ```python
    from statsmodels.tsa.trend import HodrickPrescott
    cycle, trend = HodrickPrescott(df['feature'])
    ```

38. **Lag Plots**
    ```python
    from pandas.plotting import lag_plot
    lag_plot(df['feature'])
    plt.show()
    ```

39. **Quantile-Quantile Plot (Q-Q Plot)**
    ```python
    import scipy.stats as stats
    stats.probplot(df['feature'], dist="norm", plot=plt)
    plt.show()
    ```

#### **For Discrete Features:**

40. **Frequency Table**
    ```python
    df['feature'].value_counts()
    ```

41. **Bar Plot**
    ```python
    sns.countplot(x=df['feature'])
    plt.show()
    ```

42. **Pie Chart**
    ```python
    df['feature'].value_counts().plot.pie(autopct='%1.1f%%')
    plt.show()
    ```

43. **Mode**
    ```python
    df['feature'].mode()
    ```

44. **Chi-Square Test**
    ```python
    from scipy.stats import chi2_contingency
    chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['feature1'], df['feature2']))
    ```

45. **Proportions**
    ```python
    df['feature'].value_counts(normalize=True)
    ```

46. **Contingency Table**
    ```python
    pd.crosstab(df['feature1'], df['feature2'])
    ```

47. **Cross-tabulation**
    ```python
    pd.crosstab(df['feature1'], df['feature2'])
    ```

48. **Stacked Bar Plot**
    ```python
    df.groupby(['feature1', 'feature2']).size().unstack().plot(kind='bar', stacked=True)
    plt.show()
    ```

49. **Dot Plot**
    ```python
    df['feature'].value_counts().plot(kind='dot')
    plt.show()
    ```

50. **Crosstab with Aggregation**
    ```python
    df.groupby(['feature1', 'feature2'])['numeric_feature'].agg('mean')
    ```

51. **Cross-feature Relationship**
    ```python
    sns.countplot(x='feature1', hue='feature2', data=df)
    plt.show()
    ```

52. **Benford's Law**
    ```python
    import numpy as np
    from collections import Counter
    first_digits = df['feature'].apply(lambda x: str(x)[0])
    first_digit_counts = Counter(first_digits)
    ```

53. **Fisher's Exact Test**
    ```python
    from scipy.stats import fisher_exact
    fisher_exact(pd.crosstab(df['feature1'], df['feature2']))
    ```

54. **Spearman's Rank Correlation**
    ```python
    df[['feature1', 'feature2']].corr(method='spearman')
    ```

55. **Shannon Entropy**
    ```python
    from scipy.stats import entropy
    entropy(df['feature'].value_counts(normalize=True))
    ```

56. **Cohen's Kappa**
    ```python
    from sklearn.metrics import cohen_kappa_score
    cohen_kappa_score(df['feature1'], df['feature2'])
    ```

57. **Cluster Analysis**
    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=3)
    df['cluster'] = kmeans.fit_predict(df[['feature1', 'feature2']])
    ```

58. **Association Rule Mining**
    ```python
    from mlxtend.frequent_patterns import apriori, association_rules
    frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)
    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)
    ```

59. **Mode Comparison**
    ```python
    df.groupby('feature')['numeric_feature'].agg('mode')
    ```

60. **Survival Analysis**
    ```python
    from lifelines import KaplanMeierFitter
    kmf = KaplanMeierFitter()
    kmf.fit(df['duration'], event_observed=df['event'])
    kmf.plot()
    ```

61. **Cumulative Frequency Plot**
    ```python
    df['feature'].value_counts().sort_index().cumsum().plot()
    plt.show()
    ```

62. **Odds Ratio**
    ```python
    odds_ratio = (df['feature1'].value_counts(normalize=True)[1] / df['feature1'].value_counts(normalize=True)[0]) / 
                 (df['feature2'].value_counts(normalize=True)[1] / df['feature2'].value_counts(normalize=True)[0])
    ```

63. **Conditional Probability**
    ```python
    conditional_prob = df[df['feature2'] == 'category1']['feature1'].value_counts(normalize=True)
    ```

64. **Log-Linear Model**
    ```python
    import statsmodels.api as sm
    model = sm.GLM(df['feature1'], df[['feature2', 'feature3']], family=sm.families.Poisson()).fit()
    ```

65. **Multinomial Logistic Regression**
    ```python
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
    model.fit(df[['feature1', 'feature2']], df['feature3'])
    ```

66. **Random Forest Feature Importance**
    ```python
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier()
    rf.fit(df[['feature1', 'feature2']], df['target'])
    importance = rf.feature_importances_
    ```

67. **Factorization**
    ```python
    from sklearn.decomposition import FactorAnalysis
    fa = FactorAnalysis(n_components=2)
    fa_result = fa.fit_transform(df[['feature1', 'feature2']])
    ```

68. **Multiple Correspondence Analysis (MCA)**
    ```python
    from prince import MCA
    mca = MCA(n_components=2)
    mca_result = mca.fit(df[['feature1', 'feature2', 'feature3']])
    ```

69. **Non-parametric Tests**
    ```python
    from scipy.stats import kruskal
    kruskal(df['feature1'], df['feature2'])
    ```

70. **Cluster Heatmap**
    ```python
    sns.clustermap(df[['feature1', 'feature2']], cmap='coolwarm')
    plt.show()
    ```

71. **Outlier Detection**
    ```python
    from scipy.stats import iqr
    outliers = df[df['feature'] > (df['feature'].quantile(0.75) + 1.5 * iqr(df['feature']))]
    ```

72. **Gini Index**
    ```python
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(criterion='gini')
    tree.fit(df[['feature1']], df['feature2'])
    gini = tree.tree_.impurity
    ```

73. **Permutation Test**
    ```python
    from sklearn.utils import shuffle
    df_permuted = shuffle(df[['feature1', 'feature2']])
    ```

74. **Logistic Regression**
    ```python
    from sklearn.linear_model import LogisticRegression
    logreg = LogisticRegression()
    logreg.fit(df[['feature1', 'feature2']], df['target'])
    ```


### **For Categorical Data (Discrete Features):**

75. **Logistic Regression (Binary)**
    ```python
    from sklearn.linear_model import LogisticRegression
    logreg = LogisticRegression()
    logreg.fit(df[['feature1', 'feature2']], df['binary_target'])
    ```

76. **Random Forest for Categorical Features**
    ```python
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier()
    rf.fit(df[['feature1', 'feature2']], df['target'])
    importance = rf.feature_importances_
    ```

77. **Gini Index for Feature Importance**
    ```python
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(criterion='gini')
    tree.fit(df[['feature1', 'feature2']], df['target'])
    gini = tree.feature_importances_
    ```

78. **Cross-validation**
    ```python
    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(RandomForestClassifier(), df[['feature1', 'feature2']], df['target'], cv=5)
    ```

79. **Multicollinearity Check (VIF)**
    ```python
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    vif_data = pd.DataFrame()
    vif_data['feature'] = df.columns
    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    ```

80. **ANOVA (Analysis of Variance)**
    ```python
    from scipy.stats import f_oneway
    groups = [df[df['category'] == cat]['numeric_feature'] for cat in df['category'].unique()]
    f_stat, p_val = f_oneway(*groups)
    ```

81. **Kruskal-Wallis Test**
    ```python
    from scipy.stats import kruskal
    groups = [df[df['category'] == cat]['numeric_feature'] for cat in df['category'].unique()]
    h_stat, p_val = kruskal(*groups)
    ```

82. **Kendall's Tau Correlation**
    ```python
    df[['feature1', 'feature2']].corr(method='kendall')
    ```

83. **Spearman's Rank Correlation**
    ```python
    df[['feature1', 'feature2']].corr(method='spearman')
    ```

84. **Feature Engineering (Categorical Variables)**
    ```python
    df['feature_encoded'] = df['categorical_feature'].map({'cat1': 1, 'cat2': 2, 'cat3': 3})
    ```

85. **Label Encoding**
    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    df['encoded_feature'] = le.fit_transform(df['categorical_feature'])
    ```

86. **One-Hot Encoding**
    ```python
    df_encoded = pd.get_dummies(df, columns=['categorical_feature'])
    ```

87. **Frequency Encoding**
    ```python
    freq = df['categorical_feature'].value_counts()
    df['encoded_feature'] = df['categorical_feature'].map(freq)
    ```

88. **Target Encoding**
    ```python
    target_mean = df.groupby('categorical_feature')['target'].mean()
    df['encoded_feature'] = df['categorical_feature'].map(target_mean)
    ```

89. **Count Encoding**
    ```python
    count = df['categorical_feature'].value_counts()
    df['encoded_feature'] = df['categorical_feature'].map(count)
    ```

90. **Interaction Features**
    ```python
    df['interaction'] = df['feature1'] * df['feature2']
    ```

91. **Ordinal Encoding**
    ```python
    from sklearn.preprocessing import OrdinalEncoder
    ordinal = OrdinalEncoder()
    df['encoded_feature'] = ordinal.fit_transform(df[['categorical_feature']])
    ```

92. **Mean Encoding**
    ```python
    mean_encodings = df.groupby('categorical_feature')['target'].mean()
    df['encoded_feature'] = df['categorical_feature'].map(mean_encodings)
    ```

93. **Binning Categorical Data**
    ```python
    df['binned_feature'] = pd.cut(df['numerical_feature'], bins=3, labels=['Low', 'Medium', 'High'])
    ```

94. **Multi-class Classification**
    ```python
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(multi_class='ovr', solver='lbfgs')
    model.fit(df[['feature1', 'feature2']], df['categorical_target'])
    ```

95. **Clustering Categorical Data (K-means)**
    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters=3)
    df['cluster'] = kmeans.fit_predict(df[['feature1', 'feature2']])
    ```

96. **Clustering Categorical Data (Agglomerative Clustering)**
    ```python
    from sklearn.cluster import AgglomerativeClustering
    clustering = AgglomerativeClustering(n_clusters=3)
    df['cluster'] = clustering.fit_predict(df[['feature1', 'feature2']])
    ```

97. **Cluster Profile Plot**
    ```python
    sns.pairplot(df, hue='cluster')
    plt.show()
    ```

98. **Feature Selection (Tree-based models)**
    ```python
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier()
    rf.fit(df[['feature1', 'feature2']], df['target'])
    importance = rf.feature_importances_
    plt.bar(df.columns, importance)
    plt.show()
    ```

99. **Association Rules Mining**
    ```python
    from mlxtend.frequent_patterns import apriori, association_rules
    frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True)
    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)
    ```

100. **Evaluation of Classification Model (Confusion Matrix)**
    ```python
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(df['true_labels'], df['predicted_labels'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.show()
    ```

---

### **Explanation of some techniques used above:**

- **Summary Statistics**: This includes metrics such as mean, median, standard deviation, and percentiles to summarize the data.
- **Correlation Analysis**: This helps in understanding how numerical variables are related to each other.
- **Outlier Detection**: Methods like Z-score, IQR, and visualization methods (like Boxplots) are used to detect extreme values.
- **Visualizations**: Various types of plots (like histograms, scatter plots, boxplots) are used to visualize the distribution and relationships within the data.
- **Feature Engineering**: Methods like one-hot encoding, target encoding, and mean encoding are used to convert categorical features into numerical formats.
- **Modeling Techniques**: Logistic regression, random forest, and clustering algorithms are used to analyze relationships and group data.

By applying these methods to your dataset, you will get a deeper understanding of your data.